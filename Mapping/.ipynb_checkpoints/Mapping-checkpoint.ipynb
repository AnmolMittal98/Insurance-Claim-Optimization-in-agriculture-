{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolution Neural Network\n",
    "class ConvRelu(torch.nn.Module):\n",
    "    def __init__(self, in_depth, out_depth):\n",
    "      super(ConvRelu, self).__init__()\n",
    "      self.conv = torch.nn.Conv2d(in_depth, out_depth, kernel_size=3, stride=1, padding=1)\n",
    "      self.activation = torch.nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "      x = self.conv(x)\n",
    "      x = self.activation(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decode Block - Upsizing tensor to generate images from Neural Network\n",
    "class DecoderBlock(torch.nn.Module):\n",
    "    def __init__(self, in_depth, middle_depth, out_depth):\n",
    "      super(DecoderBlock, self).__init__()\n",
    "      self.conv_relu = ConvRelu(in_depth, middle_depth)\n",
    "      self.conv_transpose = torch.nn.ConvTranspose2d(middle_depth, out_depth, kernel_size=4, stride=2, padding=1)\n",
    "      self.activation = torch.nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "      x = self.conv_relu(x)\n",
    "      x = self.conv_transpose(x)\n",
    "      x = self.activation(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNetArchitechture\n",
    "class UNetResNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "      super(UNetResNet, self).__init__()\n",
    "      \n",
    "      self.encoder = torchvision.models.resnet101(pretrained=True)\n",
    "      \n",
    "      self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "      self.conv1 = torch.nn.Sequential(self.encoder.conv1, self.encoder.bn1, self.encoder.relu, self.pool)\n",
    "      self.conv2 = self.encoder.layer1\n",
    "      self.conv3 = self.encoder.layer2\n",
    "      self.conv4 = self.encoder.layer3\n",
    "      self.conv5 = self.encoder.layer4\n",
    "      \n",
    "      self.pool = torch.nn.MaxPool2d(2, 2)      \n",
    "      self.center = DecoderBlock(2048, 512, 256)\n",
    "      \n",
    "      self.dec5 = DecoderBlock(2048 + 256, 512, 256)\n",
    "      self.dec4 = DecoderBlock(1024 + 256, 512, 256)\n",
    "      self.dec3 = DecoderBlock(512 + 256, 256, 64)\n",
    "      self.dec2 = DecoderBlock(256 + 64, 128, 128)\n",
    "      self.dec1 = DecoderBlock(128, 128, 32)\n",
    "      self.dec0 = ConvRelu(32, 32)\n",
    "      self.final = torch.nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "      conv1 = self.conv1(x)\n",
    "      conv2 = self.conv2(conv1)\n",
    "      conv3 = self.conv3(conv2)\n",
    "      conv4 = self.conv4(conv3)\n",
    "      conv5 = self.conv5(conv4)\n",
    "      pool = self.pool(conv5)\n",
    "      center = self.center(pool)\n",
    "      dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "      dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "      dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "      dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "      dec1 = self.dec1(dec2)\n",
    "      dec0 = self.dec0(dec1)\n",
    "      return self.final(dec0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch Data\n",
    "imgs = []\n",
    "masks = []\n",
    "\n",
    "trans = transforms.ToTensor()\n",
    "width = 900\n",
    "height = 500\n",
    "for num in range(8):\n",
    "    im = Image.open('./training dataset/0' + str(num+1) + '.jpg')\n",
    "    im = im.resize((width, height), Image.ANTIALIAS)\n",
    "    imgs.append(trans(im))\n",
    "    im = Image.open('./training dataset/0' + str(num+1) + ' - mask.jpg')\n",
    "    im = im.resize((width, height), Image.ANTIALIAS)\n",
    "    masks.append(trans(im))\n",
    "imgs = torch.stack(imgs)\n",
    "masks = torch.stack(masks)\n",
    "print(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 1. Got 28 and 29 in dimension 3 at /Users/distiller/project/conda/conda-bld/pytorch_1565272526878/work/aten/src/TH/generic/THTensor.cpp:689",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1e848e63b9f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munet_resnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c26490b13c55>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0mcenter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m       \u001b[0mdec5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m       \u001b[0mdec4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdec5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mdec3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdec4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 28 and 29 in dimension 3 at /Users/distiller/project/conda/conda-bld/pytorch_1565272526878/work/aten/src/TH/generic/THTensor.cpp:689"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "unet_resnet = UNetResNet(num_classes=2)\n",
    "unet_resnet.train()\n",
    "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(unet_resnet.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "\n",
    "for epoch_idx in range(2):\n",
    "    print(epoch_idx)\n",
    "    imgs = torch.autograd.Variable(imgs)\n",
    "    masks = torch.autograd.Variable(masks)\n",
    "    y = unet_resnet(imgs)\n",
    "    loss = cross_entropy_loss(y, masks)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_batches = loss.data.cpu().numpy()\n",
    "    print('epoch: ' + str(epoch_idx) + ' training loss: ' + str(loss_batches))\n",
    "\n",
    "#Saving Model\n",
    "model_file = './model'\n",
    "unet_resnet = unet_resnet.cpu()\n",
    "torch.save(unet_resnet.state_dict(), model_file)\n",
    "unet_resnet = unet_resnet.cuda()\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "unet_resnet = UNetResNet(num_classes=2)\n",
    "model_path= './model'\n",
    "pretrained_model = torch.load(model_path)\n",
    "for name, tensor in pretrained_model.items():\n",
    "    unet_resnet.state_dict()[name].copy_(tensor)\n",
    "unet_resnet.eval()\n",
    "softmax2d = torch.nn.Softmax2d()\n",
    "img = cv2.imread('./img.png')\n",
    "assert img.shape[0] % 64 == 0 and img.shape[1] % 64 == 0\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img = (img / 255.0 - MEAN) / STD\n",
    "img = img.transpose(0, 3, 1, 2)\n",
    "img = torch.FloatTensor(img)\n",
    "img = img.cuda()\n",
    "with torch.no_grad():\n",
    "    pred = unet_resnet(img)\n",
    "    pred = softmax2d(pred)\n",
    "    pred = pred[0, 1, :, :] > 0.7\n",
    "pred = pred.data.cpu().numpy()\n",
    "mask = (pred * 255.0).astype(np.uint8)\n",
    "cv2.imwrite('./mask.png', mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
